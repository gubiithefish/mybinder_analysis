{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing mybinder.org launches\n",
    "\n",
    "The first few cells download and massage the data. Later on we answer questions on which repositories are popular and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update local archive storage\n",
    "<b>1.</b> Load the up-to-date list of> the online archive files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_index = pd.read_json(\"https://archive.analytics.mybinder.org/index.jsonl\", lines=True)\n",
    "online_list = list(online_index.name)[:-1] #[:-1]\n",
    "print(f\"Count of local traffic data archives is {len(online_list)} with '{sorted(online_list)[-1:][0]}' being the newest file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.</b> Load the list of already-existing local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_index = [file for file in os.listdir('./archive')]\n",
    "print(f\"Count of local traffic data archives is {len(local_index)} with '{sorted(local_index)[-1:][0]}' being the newest file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.</b> If the two counts above does not match; compare the lists and download those that haven't been downloaded yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for online_file in enumerate(online_list):\n",
    "    if online_file[1] not in local_index:\n",
    "        # Tracking of the loop’s progress\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # The loop itself\n",
    "        with open(f\"archive/{online_file[1]}\", 'w') as local_file:\n",
    "            df = pd.read_json(f\"https://archive.analytics.mybinder.org/{online_file[1]}\",lines=True)\n",
    "            df['spec'] = df['spec'].str.replace('%2F','/')\n",
    "            df['spec'] = df['spec'].str.replace('%3A',':')\n",
    "            df['spec'] = df['spec'].str.replace('http://','')\n",
    "            df['spec'] = df['spec'].str.replace('https://','')\n",
    "            local_file.write(df.to_json(orient='index'))\n",
    "\n",
    "        # Tracking of the loop’s progress\n",
    "        print(\"Current progress:\", np.round(online_file[0]/len(online_list) * 100, 2), \"%\")\n",
    "\n",
    "# Final output\n",
    "clear_output()\n",
    "print(\"Current progress: Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the traffic data\n",
    "<b>1.</b> Define time range for which traffic data should be analyzed and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range for the analysis\n",
    "date_start = datetime(2018, 11, 3) # YYYY MM DD\n",
    "date_end   = datetime(2019, 12, 11) # YYYY MM DD\n",
    "\n",
    "# Catches any incorrect date range as it messes up the timetracking \n",
    "if date_start < datetime(2018, 11, 3):\n",
    "    date_start = datetime(2018, 11, 3)\n",
    "if date_end < datetime.today():\n",
    "    date_end = datetime.today() - timedelta(days=days_to_subtract)\n",
    "\n",
    "# Number of days of archives included in the analysis\n",
    "date_range = (date_end - date_start).days\n",
    "\n",
    "# Tracking the loop’s progress\n",
    "time_start = timeit.default_timer()\n",
    "\n",
    "# Loading loop \n",
    "frames = []\n",
    "for local_file in enumerate(local_index):\n",
    "    # Tracking the loop’s progress\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Datetime for local file\n",
    "    time_y = int(local_file[1][7:11])\n",
    "    time_m = int(local_file[1][12:14])\n",
    "    time_d = int(local_file[1][15:17])\n",
    "    \n",
    "    # If local_file datetime is within the range of the defined time range \n",
    "    if date_start < datetime(time_y, time_m, time_d) < date_end:\n",
    "        with open(f'archive/{local_file[1]}', 'r') as json_file:\n",
    "            df = pd.read_json(json_file, orient='index')\n",
    "            frames.append(df)\n",
    "\n",
    "        # Tracking the loop’s progress\n",
    "        time_stop = timeit.default_timer()\n",
    "        \n",
    "        if (local_file[0] / date_range) * 100 < 5:\n",
    "            expected_time = \"Calculating...\"\n",
    "        \n",
    "        else:\n",
    "            time_for_perc = timeit.default_timer()\n",
    "            expected_time = (time_for_perc - time_start) / (local_file[0] / date_range)\n",
    "            expected_time = str(timedelta(seconds=(expected_time)))\n",
    "        \n",
    "        print(f\"Currently fetching data for {date_range} days.\")\n",
    "        print(\"Current progress:\", np.round(local_file[0] / date_range * 100, 2), \"%\")\n",
    "        print(\"Current run time:\", str(timedelta(seconds=(time_stop - time_start))))\n",
    "        print(\"Expected run time:\", expected_time)\n",
    "\n",
    "# Final output\n",
    "clear_output()\n",
    "print(f\"Current progress: Done! Fetched data for {date_range} days\")\n",
    "print(f\"Expected run time: {expected_time}\")\n",
    "print(f\"Actual run time  : {str(timedelta(seconds=(time_stop - time_start)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.</b> Combine all the loaded dataframes, edit and transform some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation of the loaded dataframes \n",
    "df = pd.concat(frames, sort=True)\n",
    "\n",
    "# Method to simplifying the process to grab the binder-document referrence\n",
    "def get_repo(spec):\n",
    "    s = spec.rsplit(\"/\", 1)[0]\n",
    "    if s.endswith('.git'):\n",
    "        s = s[:-4]\n",
    "    return s\n",
    "\n",
    "# Seperation of the components of the binder URLs\n",
    "df['repo'] = df['spec'].apply(get_repo) #lambda s: s.rsplit(\"/\", 1)[0].replace(\".git\", \"\"))\n",
    "df['org'] = df['spec'].apply(lambda s: s.split(\"/\", 1)[0])\n",
    "df['ref'] = df['spec'].apply(lambda s: s.rsplit(\"/\", 1)[1])\n",
    "\n",
    "# Drop all unwanted columns of the dataframes\n",
    "df = df.drop(columns=['origin', 'provider', 'schema', 'spec', 'status', 'version'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.</b> Preview a sample of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly, weekly, daily active repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_active_repos(period=30):\n",
    "    \"\"\"Unique active repos over the period\n",
    "    \n",
    "    With period=30 this gives monthly active repos\n",
    "    With period=1 you get daily active repos\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    start = datetime(2019, 1, 1)\n",
    "    days_since_start = (now - start).days\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for n in range(days_since_start):\n",
    "        s = start + timedelta(days=n)\n",
    "        e = start + timedelta(days=n+period)\n",
    "        if e > now:\n",
    "            break\n",
    "\n",
    "        # our \"monthly\" data\n",
    "        monthly = df_.loc[s.isoformat():e.isoformat()]\n",
    "        data.append(dict(timestamp=e, repos=len(set(monthly.repo))))\n",
    "        \n",
    "    return pd.DataFrame.from_records(data).set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_active = n_active_repos(30)\n",
    "weekly_active = n_active_repos(7)\n",
    "daily_active = n_active_repos(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.merge(monthly_active, daily_active,\n",
    "                    suffixes=(\"_monthly\", \"_daily\"), left_index=True, right_index=True)\n",
    "activity = pd.merge(activity, weekly_active, left_index=True, right_index=True)\n",
    "\n",
    "activity.columns = ['30day active', '1d active', '7d active']\n",
    "activity.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sneak peek: total launches!\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launches per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = df.set_index(\"timestamp\").resample('D').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['repo'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate number of unique repositories\n",
    "\n",
    "Expect the raw number of launches to be bigger than the number of repositories launched more than once. Those launched only once might have been accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(df.repo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sum(1 for k,v in Counter(df.repo).items() if v > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular repositories and their branches\n",
    "\n",
    "Twenty most popular repos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20 = df.groupby(\"repo\").count().sort_values(\"timestamp\", ascending=False).head(20)\n",
    "top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cumulative top20 launches:\", top20['ref'].sum())\n",
    "print(\"The top20 repos are {:.1f}% of all \"\n",
    "      \"launches.\".format(100 * top20['ref'].sum() / df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
